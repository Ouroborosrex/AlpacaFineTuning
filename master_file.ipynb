{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T21:54:50.565110Z",
     "start_time": "2024-03-23T21:54:42.935240500Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "if major_version >= 8:\n",
    "    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
    "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:52:59.558266400Z",
     "start_time": "2024-03-25T01:52:58.011271300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLAMA2 7B Training Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 20:53:04.855219: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-24 20:53:04.883516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 20:53:05.247950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-2-7b-bnb-4bit\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:53:06.067400300Z",
     "start_time": "2024-03-25T01:53:01.996296700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39ea42e7eb3f4ae3ba4471013deda535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "def formatting_prompts_func_test(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = ['']*len(examples['input'])\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_raw = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset_raw.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "eval_dataset = dataset_raw.map(formatting_prompts_func_test, batched = True,)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:54:33.646923700Z",
     "start_time": "2024-03-25T01:54:32.472350600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "model.save_pretrained(\"llama7b_alpaca_ft\") # Local saving"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mistral Training Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mFastLanguageModel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m      2\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munsloth/mistral-7b-bnb-4bit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;66;03m# Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     max_seq_length \u001B[38;5;241m=\u001B[39m max_seq_length,\n\u001B[1;32m      4\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtype,\n\u001B[1;32m      5\u001B[0m     load_in_4bit \u001B[38;5;241m=\u001B[39m load_in_4bit,\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\u001B[39;00m\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m FastLanguageModel\u001B[38;5;241m.\u001B[39mget_peft_model(\n\u001B[1;32m     10\u001B[0m     model,\n\u001B[1;32m     11\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m, \u001B[38;5;66;03m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     20\u001B[0m     loftq_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;66;03m# And LoftQ\u001B[39;00m\n\u001B[1;32m     21\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'FastLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:05:39.832011800Z",
     "start_time": "2024-03-23T22:05:39.809168100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m alpaca_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\u001B[39m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m### Instruction:\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m### Response:\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m EOS_TOKEN \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39meos_token \u001B[38;5;66;03m# Must add EOS_TOKEN\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformatting_prompts_func\u001B[39m(examples):\n\u001B[1;32m     14\u001B[0m     instructions \u001B[38;5;241m=\u001B[39m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstruction\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:05:22.642867400Z",
     "start_time": "2024-03-23T22:05:22.453782300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5654c9d0e2954ddc8ebfa406a95d2d70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/60 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "model.save_pretrained(\"lora_mistral\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phi2 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n",
    "!pip install einops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:55:23.379012200Z",
     "start_time": "2024-03-25T01:55:23.327762700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Input:\\n\\n\\n### Response:\\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>'}\n"
     ]
    }
   ],
   "source": [
    " # Model\n",
    "base_model = \"microsoft/phi-2\"\n",
    "#Fine-tune model name\n",
    "new_model = \"phi2-2b-alpaca\"\n",
    "#Load the Dataset from hugging face\n",
    "#Tokenizer\n",
    "#Load the tokenizer from Llama 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "#In Llama2 we dont have the padding token which is a very big problem, because we have a dataset with different number of tokens in each row.\n",
    "#So, we need to pad it so they all have the same length and here i am using end of sentence token and this will have an impact on the generation of our model\n",
    "#I am using End of Sentence token for fine-tuning\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\"\n",
    "\n",
    "print(dataset[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:55:25.282842300Z",
     "start_time": "2024-03-25T01:55:25.160208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/16017 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01dca3ab94764e049fc8d7858cbd9444"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4005 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be16766872794830ae33db81f3530cc2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "def formatting_prompts_func_test(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = [''] * len(examples[\"input\"])\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_raw = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "\n",
    "# Calculate the number of samples in the training set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "\n",
    "# Create the testing dataset\n",
    "test_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "test_dataset = test_dataset.map(formatting_prompts_func_test, batched = True,)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:33:54.805782100Z",
     "start_time": "2024-03-25T01:33:53.304178700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2feeb0e79534a86a9a21d79fc216fb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "#Configration of QLoRA\n",
    "#Quantization Configuration\n",
    "#To reduce the VRAM usage we will load the model in 4 bit precision and we will do quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    #Quant type\n",
    "    #We will use the \"nf4\" format this was introduced in the QLoRA paper\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    #As the model weights are stored using 4 bits and when we want to compute its only going to use 16 bits so we have more accuracy\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    #Quantization parameters are quantized\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ] # [\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\" ], - 41M params\n",
    "    # modules_to_save=[\"embed_tokens\",\"lm_head\"]\n",
    ")\n",
    "\n",
    "# Load base moodel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    # use_flash_attention_2=True, # Phi does not support yet.\n",
    "    trust_remote_code=True,\n",
    "    flash_attn=True,\n",
    "    flash_rotary=True,\n",
    "    fused_dense=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map={\"\": 0},\n",
    "    revision=\"refs/pr/23\",\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
    "#prepare_model_for_kbit_training---> This function basically helps to built the best model possible\n",
    "model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T01:36:04.366383800Z",
     "start_time": "2024-03-25T01:35:55.156899300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "\n",
    "# Set training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,#3,5 good for the Llama 2 Model\n",
    "        per_device_train_batch_size=8,# Number of batches that we are going to take for every step\n",
    "        gradient_accumulation_steps=32,\n",
    "        evaluation_strategy=\"steps\",#Not helpful because we donot want to evaluate the model we just want to train it\n",
    "        eval_steps=2000,\n",
    "        logging_steps=25,\n",
    "        optim=\"paged_adamw_8bit\",#Adam Optimizer we will be using but a version that is paged and in 8 bits, so it will lose less memory\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=10,\n",
    "        warmup_ratio=0.05,\n",
    "        # report_to=\"tensorboard\",\n",
    "        weight_decay=0.01,\n",
    "        max_steps=-1, # if maximum steps=2, it will stop after two steps\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,#No separate evaluation dataset, i am using the same dataset\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,# In dataset creation we put a threshold 2k for context length (input token limit) but we dont have enough VRAM unfortunately it will take a lot of VRAM to put everything into memory so we are just gonna stop at 512\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLaMA2 Eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n"
     ]
    }
   ],
   "source": [
    "def make_inst(dataset):\n",
    "  instr_list = []\n",
    "\n",
    "  for i in range(len(dataset)):\n",
    "    instr_list.append(alpaca_prompt.format(\n",
    "        dataset[i]['instruction'], # instruction\n",
    "        dataset[i]['input'], # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ))\n",
    "  print(len(instr_list))\n",
    "  return instr_list\n",
    "\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_llama7b_alpaca_ft\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "list_dataset = make_inst(dataset)\n",
    "llama_responses = []\n",
    "num_samples = 20\n",
    "\n",
    "for i in range(num_samples):\n",
    "    inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    llama_responses.append(tokenizer.batch_decode(outputs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:15:54.631758200Z",
     "start_time": "2024-03-25T02:15:06.549236700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mistral eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_mistral\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "# dataset = make_inst(dataset)\n",
    "mistral_responses = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    mistral_responses.append(tokenizer.batch_decode(outputs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:16:40.871999Z",
     "start_time": "2024-03-25T02:15:54.631758200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6c2dd0922d847718848f0bf99cf84ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "#Reload the Base Model and load the QLoRA adapters\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "# Run text generation pipeline with our model\n",
    "#Input Prompt\n",
    "\n",
    "#prompt = \"What is a large language model?\"\n",
    "#Wrap the prompt using the right chat template\n",
    "phi_responses = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    phi_responses.append(tokenizer.batch_decode(outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:17:02.920634500Z",
     "start_time": "2024-03-25T02:16:40.874997100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting rouge\r\n",
      "  Using cached rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Collecting bert-score\r\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: click in /home/brycelinux/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /home/brycelinux/.local/lib/python3.10/site-packages (from nltk) (1.3.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/brycelinux/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\r\n",
      "Requirement already satisfied: tqdm in /home/brycelinux/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\r\n",
      "Requirement already satisfied: six in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from rouge) (1.16.0)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from bert-score) (2.2.1)\r\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/brycelinux/.local/lib/python3.10/site-packages (from bert-score) (2.0.3)\r\n",
      "Requirement already satisfied: transformers>=3.0.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from bert-score) (4.39.1)\r\n",
      "Requirement already satisfied: numpy in /home/brycelinux/.local/lib/python3.10/site-packages (from bert-score) (1.23.5)\r\n",
      "Requirement already satisfied: requests in /home/brycelinux/.local/lib/python3.10/site-packages (from bert-score) (2.31.0)\r\n",
      "Requirement already satisfied: matplotlib in /home/brycelinux/.local/lib/python3.10/site-packages (from bert-score) (3.7.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /home/brycelinux/.local/lib/python3.10/site-packages (from bert-score) (23.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/brycelinux/.local/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/brycelinux/.local/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/brycelinux/.local/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3)\r\n",
      "Requirement already satisfied: filelock in /home/brycelinux/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.10.0)\r\n",
      "Requirement already satisfied: sympy in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.12)\r\n",
      "Requirement already satisfied: networkx in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/brycelinux/.local/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.2.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.99)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.21.4)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/brycelinux/.local/lib/python3.10/site-packages (from matplotlib->bert-score) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from matplotlib->bert-score) (4.50.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from matplotlib->bert-score) (10.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from matplotlib->bert-score) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/brycelinux/.local/lib/python3.10/site-packages (from requests->bert-score) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/brycelinux/.local/lib/python3.10/site-packages (from requests->bert-score) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/brycelinux/.local/lib/python3.10/site-packages (from requests->bert-score) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/brycelinux/.local/lib/python3.10/site-packages (from requests->bert-score) (2023.5.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/brycelinux/.local/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/brycelinux/miniconda3/envs/pythonProject2/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\r\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\r\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\r\n",
      "Installing collected packages: rouge, nltk, bert-score\r\n",
      "Successfully installed bert-score-0.3.13 nltk-3.8.1 rouge-1.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk rouge bert-score\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T00:46:33.153687700Z",
     "start_time": "2024-03-25T00:46:31.056812600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.62821      | 0.80228   \n",
      "Mistral    | 0.00000    | 0.63366      | 0.79927   \n",
      "Phi2       | 0.00000    | 0.63741      | 0.80384   \n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "\n",
    "# llama_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "# mistral_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "# phi_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "model_scores = {'LLaMA2 7B':{'bleu':[],'rouge':[],'BertF1':[]},'Mistral':{'bleu':[],'rouge':[],'BertF1':[]},'Phi2':{'bleu':[],'rouge':[],'BertF1':[]}}\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    # LLaMA2 Evaluation Code\n",
    "    bleu_score = sentence_bleu(dataset[i]['text'].split(' '), llama_responses[i][0].split(' '))\n",
    "    # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "    rouge = Rouge()\n",
    "\n",
    "    rouge_scores = rouge.get_scores(dataset[i]['text'], llama_responses[i][0], avg=True)\n",
    "    # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "    P, R, F1 = bert_score([dataset[i]['text']], llama_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "    model_scores['LLaMA2 7B']['bleu'].append(bleu_score)\n",
    "    model_scores['LLaMA2 7B']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "    model_scores['LLaMA2 7B']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "    bleu_score = sentence_bleu(dataset[i]['text'].split(' '), mistral_responses[i][0].split(' '))\n",
    "    # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "    rouge_scores = rouge.get_scores(dataset[i]['text'], mistral_responses[i][0], avg=True)\n",
    "    # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "    P, R, F1 = bert_score([dataset[i]['text']], mistral_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "    model_scores['Mistral']['bleu'].append(bleu_score)\n",
    "    model_scores['Mistral']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "    model_scores['Mistral']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "    bleu_score = sentence_bleu(dataset[i]['text'].split(' '), phi_responses[i][0].split(' '))\n",
    "\n",
    "    rouge_scores = rouge.get_scores(dataset[i]['text'], phi_responses[i][0], avg=True)\n",
    "    # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "    P, R, F1 = bert_score([dataset[i]['text']], phi_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "    model_scores['Phi2']['bleu'].append(bleu_score)\n",
    "    model_scores['Phi2']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "    model_scores['Phi2']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "\n",
    "print(f\"{'Model':<10} | {'Bleu Score':<10} | {'Rouge Score':<12} | {'BertF1':<10}\")\n",
    "print('-'*45)\n",
    "for model, scores in model_scores.items():\n",
    "    print(f\"{model:<10} | {np.array(scores['bleu']).mean():<10.5f} | {np.array(scores['rouge']).mean():<12.5f} | {np.array(scores['BertF1']).mean():<10.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:17:23.272561Z",
     "start_time": "2024-03-25T02:17:02.924139200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Input:\\n\\n\\n### Response:\\n1. Eat a balanced diet: A balanced diet includes a variety of foods from all the main food groups, including fruits, vegetables, whole grains, lean protein, and dairy.\\n\\n2. Exercise regularly: Regular physical activity is essential for maintaining'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Input:\\n\\n\\n### Response:\\n1. Eat a balanced diet: A balanced diet is essential for maintaining good health. It should include a variety of fruits, vegetables, whole grains, lean proteins, and healthy fats.\\n\\n2. Exercise regularly: Regular exercise can help improve cardiovascular health, strengthen muscles and bones,'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Input:\\n\\n\\n### Response:\\n1. Eat a balanced and nutritious diet.\\n2. Exercise regularly to maintain physical fitness.\\n3. Get enough sleep to allow your body to rest and recharge.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Input:\\n\\n\\n### Response:\\nThe three primary colors are red, yellow, and blue.</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Input:\\n\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow. These colors are the foundation of the color wheel and are used to create all other colors.</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Input:\\n\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\n",
      "\n",
      "The nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\n",
      "\n",
      "Surrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \n",
      "\n",
      "In a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Input:\\n\\n\\n### Response:\\nAn atom is the smallest unit of an element that can exist independently. An atom consists of a central nucleus, surrounded by a cloud of electrons. The nucleus is made up of protons and neutrons, while the electrons orbit around the nucleus in shells. The number of protons and electrons determines'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Input:\\n\\n\\n### Response:\\nAn atom is the smallest unit of matter that can exist independently and is made up of three main components: protons, neutrons, and electrons. Protons and neutrons are located in the nucleus, which is the dense center of the atom, while electrons orbit the nucleus in shells or energy'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Input:\\n\\n\\n### Response:\\nAn atom is composed of a nucleus, which contains protons and neutrons, and electrons that orbit the nucleus in shells or energy levels. The number of protons in the nucleus determines the element of the atom, while the number of electrons determines its charge. The electrons in the outermost shell are responsible for the chemical'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How can we reduce air pollution?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "There are several ways to reduce air pollution, including:\n",
      "\n",
      "1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\n",
      "\n",
      "2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\n",
      "\n",
      "3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\n",
      "\n",
      "4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\n",
      "\n",
      "5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\n",
      "\n",
      "6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Input:\\n\\n\\n### Response:\\nThere are several ways to reduce air pollution. Here are some of the most effective measures:\\n\\n1. Encourage Alternative Transportation: One of the most effective ways to reduce air pollution is to encourage alternative transportation methods such as walking, cycling, or using public transportation. This'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Input:\\n\\n\\n### Response:\\nThere are several ways to reduce air pollution, including:\\n\\n1. Reducing the use of fossil fuels: Fossil fuels such as coal, oil, and natural gas are major contributors to air pollution. Reducing their use by switching to renewable energy sources such as solar, wind'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Input:\\n\\n\\n### Response:\\nWe can reduce air pollution by using renewable energy sources, reducing vehicle emissions, and implementing stricter regulations on industrial emissions.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the clients expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the teams resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the clients expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n### Input:\\n\\n\\n### Response:\\nAs a project manager, one of the most difficult decisions I had to make was to terminate a contract with a subcontractor. The subcontractor was not performing to the standards we expected and was causing delays in the project. After careful consideration, we decided to terminate the contract and find a new subcont'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n### Input:\\n\\n\\n### Response:\\nAs a project manager of a construction company, I have had to make many difficult decisions throughout my career. One such decision that stands out to me was when I had to decide whether to continue with a project or terminate it due to budget constraints.\\n\\nThe project in question was a large-scale renovation of'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n\\n### Input:\\n\\n\\n### Response:\\nAs a project manager of a construction company, I have had to make difficult decisions on numerous occasions. One such instance was when we were working on a project that had a tight deadline. We had to decide whether to hire additional workers to speed up the process or to work overtime to meet the deadline. After careful consideration,'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a concise summary of the following:\n",
      "\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \n",
      "\n",
      "Additionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The Commodore 64 was a highly successful 8-bit home computer manufactured by Commodore Business Machine (CBM) in 1982, with sales amounting to approximately 17 million units sold between 1983-1986. It dominated the market with between 30% and 40% share and outsold its competitors, including IBM PC clones, Apple Computers, and Atari computers. At its peak, CBM was building 400,000 C64s a month for a couple of years.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n### Input:\\n\\n\\n### Response:\\nThe Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n### Input:\\n\\n\\n### Response:\\nThe Commodore 64 was a popular 8-bit home computer manufactured by Commodore Business Machine (CBM) in August 1982. It was known for its remarkable market success, with sales of about 17 million units sold between 1983 and 19'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n\\n### Input:\\n\\n\\n### Response:\\n\\nThe Commodore 64, also known as the C64 or CBM 64, was a popular 8-bit home computer manufactured by Commodore Business Machine (CBM) in August 1982. It was initially priced at $595 and quickly gained market success. Between 1983-1986, C64 sales reached a staggering 17 million units'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain why the following fraction is equivalent to 1/4\n",
      "\n",
      "### Input:\n",
      "4/16\n",
      "\n",
      "### Response:\n",
      "The fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. A fraction can be simplified by dividing both the numerator and the denominator by a common factor. In this case, 4 is a common factor of both the numerator and the denominator of 4/16. When we divide both by 4, we get 4/4 = 1 and 16/4 = 4, so the simplified fraction is 1/4. Alternatively, we can think of this in terms of multiplication. For example, if we multiply the numerator and denominator of the fraction 1/4 by 4, we get (1x4)/(4x4), or 4/16. Since both fractions can be derived from the other through multiplication or division by the same number, they represent the same value and are equivalent.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain why the following fraction is equivalent to 1/4\\n\\n### Input:\\n4/16\\n\\n### Response:\\nThe fraction 4/16 can be simplified to 1/4. To see why this is the case, consider the numerator 4, which is the same as the denominator 16. This means that we have an equivalent fraction, 1/4.</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain why the following fraction is equivalent to 1/4\\n\\n### Input:\\n4/16\\n\\n### Response:\\nTo determine if two fractions are equivalent, we need to find a common denominator and simplify the fractions. In this case, the common denominator is 16. When we simplify 4/16, we get 1/4. Therefore, 4/16 is equivalent to'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain why the following fraction is equivalent to 1/4\\n\\n### Input:\\n4/16\\n\\n### Response:\\nThe fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. When you divide both the numerator and denominator of 4/16 by 4, you get 1/4. This means that 4/16 and 1/4 are equal fractions.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a short story in third person narration about a protagonist who has to make an important career decision.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Sophie sat at her desk, staring blankly at the computer screen. Her mind was racing as she weighed the options in front of her. She had been offered a promotion to a senior management position within her company, but it meant moving across the country and leaving her friends and family behind. At the same time, her dream job at a rival company had finally opened up, and she had been invited for an interview. \n",
      "\n",
      "The thought of leaving her comfortable life behind, to take a chance on something new and uncertain, made Sophie feel anxious, but also excited. She had always been one to take risks, and it had paid off in the past. But now, with such a big decision to make, Sophie couldn't help but feel overwhelmed.\n",
      "\n",
      "After much deliberation, Sophie made her choice. She packed her bags, said goodbye to her loved ones, and moved across the country for the new job. It was a difficult first few months, adjusting to a new city and a new company culture. But as time went by, Sophie realized that she had made the best decision for her career. The job was challenging, but also rewarding, and she was surrounded by passionate, driven colleagues who inspired her to grow and develop.\n",
      "\n",
      "In the end, Sophie's bold move paid off. She was thriving in her new role, and had made new friends and connections in her new city. Looking back, Sophie was grateful for the opportunity to take a risk and make a change, and was confident that she had made the right decision for her career.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n### Input:\\n\\n\\n### Response:\\nThe protagonist in this story is a young professional who is faced with an important career decision. After much deliberation, they decide to take a leap of faith and accept an offer for a new job in a different field. Although they are nervous about the unknown, they are excited to start this new chapter in'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n### Input:\\n\\n\\n### Response:\\nThe protagonist, a young woman named Sarah, had always dreamed of becoming a doctor. She had worked hard throughout her college years, studying long hours and interning at a local hospital. But as she neared the end of her studies, she began to have doubts.\\n\\nSarah had always been'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\\n\\n### Input:\\n\\n\\n### Response:\\n## INPUT\\n\\n##OUTPUT\\nOnce upon a time, there was a young woman named Sarah who had just graduated from college. She had always dreamed of becoming a doctor, but after months of job searching, she realized that the medical field was not the right fit for her. She was torn between pursuing'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Evaluate this sentence for spelling and grammar mistakes\n",
      "\n",
      "### Input:\n",
      "He finnished his meal and left the resturant\n",
      "\n",
      "### Response:\n",
      "There are two spelling errors in the sentence. The corrected sentence should be: \"He finished his meal and left the restaurant.\"</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEvaluate this sentence for spelling and grammar mistakes\\n\\n### Input:\\nHe finnished his meal and left the resturant\\n\\n### Response:\\nHe finished his meal and left the restaurant.</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEvaluate this sentence for spelling and grammar mistakes\\n\\n### Input:\\nHe finnished his meal and left the resturant\\n\\n### Response:\\nThe sentence \"He finnished his meal and left the resturant\" contains a spelling mistake in the word \"resturant\". The correct spelling is \"restaurant\".</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nEvaluate this sentence for spelling and grammar mistakes\\n\\n### Input:\\nHe finnished his meal and left the resturant\\n\\n### Response:\\nHe finished his meal and left the restaurant.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How did Julius Caesar die?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Julius Caesar, the Roman Military general, and statesman was assassinated on March 15, 44 BC. He was stabbed to death at a meeting of the Senate in the Theatre of Pompey by a group of senators who called themselves the Liberatores. Led by Gaius Cassius Longinus, and Marcus Junius Brutus, they believed that Caesar's dictatorship was subverting the Roman Republic and sought to restore the traditional republican government. The exact number of assailants is not known, but it is believed that a group of as many as 60 conspirators participated in the assassination, and each of them took turns in stabbing Caesar. The day of his death is still known as the Ides of March.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow did Julius Caesar die?\\n\\n### Input:\\n\\n\\n### Response:\\nJulius Caesar was assassinated by a group of conspirators on the Ides of March in 44 BC. He was stabbed 23 times by a group of men who included Marcus Junius Brutus and Gaius Cassius Longinus. Caesar was a powerful and'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow did Julius Caesar die?\\n\\n### Input:\\n\\n\\n### Response:\\nJulius Caesar was assassinated on the Ides of March (March 15) in the year 44 BC. He was stabbed 23 times by a group of conspirators led by Brutus and Cassius. The assassination took place in the Senate House of the Roman'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow did Julius Caesar die?\\n\\n### Input:\\n\\n\\n### Response:\\nJulius Caesar was assassinated on March 15, 44 BC by a group of senators led by Brutus and Cassius.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the capital of France?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The capital city of France is Paris.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of France?\\n\\n### Input:\\n\\n\\n### Response:\\nThe capital of France is Paris.</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of France?\\n\\n### Input:\\n\\n\\n### Response:\\nThe capital of France is Paris.</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the capital of France?\\n\\n### Input:\\n\\n\\n### Response:\\nThe capital of France is Paris.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a list of ten items a person might need for a camping trip\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Here are ten items a person might need for a camping trip:\n",
      "\n",
      "1. Tent - to provide shelter and protection from the elements\n",
      "2. Sleeping bag - to stay warm and comfortable while sleeping\n",
      "3. Portable stove or campfire grill - for cooking meals\n",
      "4. Cooler with ice or ice packs - to keep perishable food and drinks cold\n",
      "5. Lantern or flashlight - to provide light during the night\n",
      "6. First aid kit - for minor injuries or ailments\n",
      "7. Map and compass or GPS - for navigation on hikes or exploring the area\n",
      "8. Camp chairs or folding chairs - for comfortable seating around the campsite\n",
      "9. Insect repellent - to protect against bug bites\n",
      "10. Sunscreen - to protect against sunburn.</s> \n",
      "\n",
      "LLaMA2 7B Response: [\"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a list of ten items a person might need for a camping trip\\n\\n### Input:\\n\\n\\n### Response:\\n1. Tent: This is the most essential item for camping. It provides shelter from the elements and can be used as a base for your camping activities.\\n\\n2. Sleeping bag: A comfortable sleeping bag is essential for a good night's sleep in the outdoors.\\n\"] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a list of ten items a person might need for a camping trip\\n\\n### Input:\\n\\n\\n### Response:\\n1. Tent\\n2. Sleeping bag\\n3. Sleeping pad\\n4. Camping stove\\n5. Cooking utensils\\n6. Food and water\\n7. Flashlight\\n8. First aid kit\\n9. Fire starter\\n10. Camping chair</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a list of ten items a person might need for a camping trip\\n\\n### Input:\\n\\n\\n### Response:\\nA person might need the following ten items for a camping trip: \\n1. Tent\\n2. Sleeping bag\\n3. Flashlight\\n4. Camping stove\\n5. Cutlery and utensils\\n6. Water container\\n7. Matches or lighter\\n8. Compass\\n9. First aid'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Discuss the causes of the Great Depression\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The Great Depression was a period of economic decline that lasted from 1929-1939, making it the longest-lasting depression in modern history. It affected not only the United States but countries around the world, causing substantial social and economic upheaval.\n",
      "\n",
      "There were several major causes of the Great Depression, which include:\n",
      "\n",
      "1. Stock Market Crash of 1929 - In October of 1929, the stock market experienced a significant fall that wiped out millions of investors. This event is considered by many to be the initial trigger of the Great Depression.\n",
      "\n",
      "2. Overproduction - During the 1920s, many industries produced more goods than consumers wanted or could afford. This ultimately led to a decline in demand for goods, causing job loss, lower wages, and business failure.\n",
      "\n",
      "3. High Tariffs and War Debts - Protectionist trade policies in the form of high tariffs led to a decline in global trade, as other countries retaliated with tariffs of their own. Additionally, many countries were struggling to repay war debts, which led to economic instability.\n",
      "\n",
      "4. Bank Failures - As demand for goods declined, many banks began to fail, causing a loss of confidence in the banking system. This led to a massive withdrawal of money from banks, causing even more banks to fail.\n",
      "\n",
      "5. Drought Conditions - The Dust Bowl was a severe drought and dust storm that hit the Great Plains region of the United States in the 1930s. This had a significant impact on agriculture, causing many farmers to lose their land and livelihoods which worsened the effects of the depression.\n",
      "\n",
      "Overall, the Great Depression was caused by a combination of factors, including economic, environmental, and political factors. Its impact was widespread, affecting millions of people around the world.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the causes of the Great Depression\\n\\n### Input:\\n\\n\\n### Response:\\nThe Great Depression was a severe worldwide economic depression that lasted from 1929 to 1939. It was the longest, deepest, and most widespread depression of the 20th century. The Great Depression was caused by a combination of factors,'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the causes of the Great Depression\\n\\n### Input:\\n\\n\\n### Response:\\nThe Great Depression was a severe economic downturn that lasted from 1929 to 1939 and was the longest and most severe economic depression in the history of the United States. The causes of the Great Depression are complex and multifaceted, but some of the key factors that contributed'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDiscuss the causes of the Great Depression\\n\\n### Input:\\n\\n\\n### Response:\\nThe Great Depression was caused by a variety of factors. The stock market crash of 1929, which led to a sharp decline in stock prices and a loss of confidence in the financial system, was a major trigger. Additionally, a combination of overproduction, unequal distribution of wealth, and high levels of debt contributed to the economic'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Classify the following into animals, plants, and minerals\n",
      "\n",
      "### Input:\n",
      "Oak tree, copper ore, elephant\n",
      "\n",
      "### Response:\n",
      "Animals: Elephant\n",
      "Plants: Oak tree\n",
      "Minerals: Copper ore</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nClassify the following into animals, plants, and minerals\\n\\n### Input:\\nOak tree, copper ore, elephant\\n\\n### Response:\\n- Oak tree: Plant\\n- Copper ore: Mineral\\n- Elephant: Animal</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nClassify the following into animals, plants, and minerals\\n\\n### Input:\\nOak tree, copper ore, elephant\\n\\n### Response:\\nOak tree: Plant\\nCopper ore: Mineral\\nElephant: Animal</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nClassify the following into animals, plants, and minerals\\n\\n### Input:\\nOak tree, copper ore, elephant\\n\\n### Response:\\nThe oak tree is a plant, the copper ore is a mineral, and the elephant is an animal.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain the use of word embeddings in Natural Language Processing\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Word embeddings are a type of natural language processing technique used to map words or phrases from a vocabulary to vectors of real numbers. The idea is to represent words in a continuous vector space, where the similarity between words can be measured by the distance between their corresponding vectors. This can be done using algorithms such as Word2Vec or GloVe, among others.\n",
      "\n",
      "The use of word embeddings has revolutionized the field of Natural Language Processing, as it allows computers to understand text in a much more meaningful way than simply looking at the presence or absence of individual words. For example, when using word embeddings, the computer can understand that the words \"dog\" and \"puppy\" are closely related, while the words \"dog\" and \"umbrella\" are not.\n",
      "\n",
      "Word embeddings are used in a wide variety of NLP tasks, including text classification, sentiment analysis, machine translation, named entity recognition, and many others. They form the basis for many advanced language processing techniques, such as deep learning and neural machine translation.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain the use of word embeddings in Natural Language Processing\\n\\n### Input:\\n\\n\\n### Response:\\nWord embeddings are a technique used in Natural Language Processing (NLP) to represent words as vectors in a high-dimensional space. These vectors are trained using large corpora of text data and capture the semantic and syntactic relationships between words. Word embeddings are used for various NLP tasks,'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain the use of word embeddings in Natural Language Processing\\n\\n### Input:\\n\\n\\n### Response:\\nWord embeddings are a technique used in Natural Language Processing (NLP) to represent words as vectors in a low-dimensional space. The goal of word embeddings is to capture the semantic and syntactic relationships between words in a way that can be used for various NLP tasks such as text classification, sentiment'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nExplain the use of word embeddings in Natural Language Processing\\n\\n### Input:\\n\\n\\n### Response:\\nWord embeddings are vector representations of words in a high-dimensional space. They are used in Natural Language Processing to capture semantic relationships between words and to enable machines to understand the meaning of text. Word embeddings are typically generated using deep learning techniques, such as Word2Vec or GloVe, which'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the function of a computer motherboard\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The motherboard, also known as the mainboard or system board, is the central printed circuit board in a computer. It serves as the backbone or foundation for a computer, connecting all the different components such as the CPU, RAM, storage drives, expansion cards, and peripherals. The motherboard manages communication and data transfer between these components, allowing them to work together and perform their designated tasks.\n",
      "\n",
      "The motherboard also includes important circuitry such as the power regulation circuit that provides power to the different components, and the clock generator which synchronizes the operation of these components. It also contains the BIOS (basic input/output system), which is a firmware that controls the boot process and provides an interface for configuring and managing the computer's hardware. Other features on a motherboard may include built-in networking, audio, and video capabilities.\n",
      "\n",
      "Overall, the function of a computer motherboard is to provide a platform for the integration and operation of all the various components that make up a computer, making it an essential part of any computing system.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the function of a computer motherboard\\n\\n### Input:\\n\\n\\n### Response:\\nA computer motherboard, also known as the system board or mainboard, is the central component of a computer that connects all the other components together. It serves as the central control unit, connecting the CPU, RAM, storage devices, and input/output (I/O) ports.\\n\\nThe motherboard'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the function of a computer motherboard\\n\\n### Input:\\n\\n\\n### Response:\\nA computer motherboard is the main circuit board in a computer system that connects and provides power to all the other components of the computer. It is the central hub of the computer, and all the other components, such as the CPU, RAM, graphics card, and hard drive, are connected to it. The mother'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the function of a computer motherboard\\n\\n### Input:\\n\\n\\n### Response:\\nA computer motherboard is the main circuit board of a computer system. It connects and communicates between all the other components of the computer, such as the CPU, RAM, and storage devices. The motherboard also contains the BIOS (Basic Input/Output System), which is responsible for initializing the computer and loading the operating system.'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Refactor this code and add comments.\n",
      "\n",
      "### Input:\n",
      "def factorialize(num):\n",
      "  factorial = 1\n",
      "  for i in range(1, num):\n",
      "    factorial *= i\n",
      "  \n",
      "  return factorial\n",
      "\n",
      "### Response:\n",
      "```\n",
      "# This is a function that calculates the factorial of a given number\n",
      "def factorialize(num):\n",
      "  # Set the variable 'factorial' to 1\n",
      "  factorial = 1\n",
      "  # Iterate from 1 to 'num' using the range function\n",
      "  for i in range(1, num):\n",
      "    # Multiply the 'factorial' variable by the current number in the iteration\n",
      "    factorial *= i\n",
      "  # Return the final value of 'factorial'\n",
      "  return factorial\n",
      "```</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRefactor this code and add comments.\\n\\n### Input:\\ndef factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial\\n\\n### Response:\\n#!/usr/bin/python\\n# -*- coding: utf-8 -*-\\n\\ndef factorialize(num):\\n    factorial = 1\\n    for i in range(1, num):\\n        factorial *= i\\n\\n    return factorial</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRefactor this code and add comments.\\n\\n### Input:\\ndef factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial\\n\\n### Response:\\n# This function calculates the factorial of a given number\\ndef factorialize(num):\\n  # Initialize the factorial to 1\\n  factorial = 1\\n  # Loop from 1 to the given number\\n  for i in range(1, num):\\n    # Multip'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRefactor this code and add comments.\\n\\n### Input:\\ndef factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial\\n\\n### Response:\\ndef factorialize(num):\\n \\n  # Initialize the factorial variable to 1\\n  factorial = 1\\n \\n  # Iterate through the range of numbers from 1 to num\\n  for i in range(1, num):\\n    # Multiply the factorial variable by'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Propose an ethical solution to the problem of data privacy\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "One ethical solution to the problem of data privacy is to enforce stricter regulations and standards that require companies to be transparent about how they collect, store, and use user data. This means that companies must explicitly state what data they are collecting from users, why they need this data, and how they plan to use it. Users will also have the right to access, modify, and delete their data at any time.\n",
      "\n",
      "In addition, companies should prioritize implementing robust security measures to protect user data from unauthorized access and breaches. This could include measures such as encryption, multi-factor authentication, and regular security audits. In the event of a data breach, companies must be held accountable and must inform affected users in a timely manner.\n",
      "\n",
      "Moreover, users should be empowered to make informed decisions about their data and privacy. This can be achieved by raising awareness about data privacy and providing tools and resources to help individuals better manage their personal information online.\n",
      "\n",
      "Ultimately, ensuring data privacy is a shared responsibility between companies and users. Companies must be committed to safeguarding user data, while users must take an active role in protecting their personal information. By working together, we can create a safer and more ethical data landscape.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPropose an ethical solution to the problem of data privacy\\n\\n### Input:\\n\\n\\n### Response:\\nEthical solutions to the problem of data privacy can involve various approaches, including:\\n\\n1. Transparency: Companies should be transparent about how they collect, use, and store personal data. They should clearly explain their data collection practices, including the types of data they collect and how it is'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPropose an ethical solution to the problem of data privacy\\n\\n### Input:\\n\\n\\n### Response:\\nOne ethical solution to the problem of data privacy is to implement a system of data minimization. This means that companies should only collect and store the minimum amount of data necessary to provide their services or products. This reduces the risk of data breaches and ensures that the data collected is only used for its intended purpose.\\n'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nPropose an ethical solution to the problem of data privacy\\n\\n### Input:\\n\\n\\n### Response:\\nOne ethical solution to the problem of data privacy is to implement strict regulations and guidelines for data collection and usage. This can include obtaining informed consent from individuals before collecting their data, limiting the amount of data that is collected, and ensuring that data is stored securely and only used for its intended purpose. Additionally, companies should be'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate three verbs that mean the same as \"to apologize\"\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. To express regret\n",
      "2. To atone\n",
      "3. To make amends</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate three verbs that mean the same as \"to apologize\"\\n\\n### Input:\\n\\n\\n### Response:\\n1. Say sorry\\n2. Apologize\\n3. Express regret</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate three verbs that mean the same as \"to apologize\"\\n\\n### Input:\\n\\n\\n### Response:\\n1. To express regret or remorse for one\\'s actions or words.\\n2. To offer an apology for something one has done or said that has caused harm or offense.\\n3. To make amends for one\\'s actions or words by expressing regret and offering an apology.</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate three verbs that mean the same as \"to apologize\"\\n\\n### Input:\\n\\n\\n### Response:\\n1. Express regret\\n2. Say sorry\\n3. Make amends\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Ground Truth: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What does DNA stand for?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "DNA stands for Deoxyribonucleic Acid. It is the molecule that carries the genetic instructions used in the growth, development, functioning, and reproduction of all living organisms.</s> \n",
      "\n",
      "LLaMA2 7B Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat does DNA stand for?\\n\\n### Input:\\n\\n\\n### Response:\\nDNA stands for deoxyribonucleic acid. It is a molecule that carries the genetic instructions used in the development and functioning of all known living organisms and many viruses.</s>'] \n",
      "\n",
      "Mistral Response: ['<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat does DNA stand for?\\n\\n### Input:\\n\\n\\n### Response:\\nDNA stands for Deoxyribonucleic Acid.</s>'] \n",
      "\n",
      "Phi2 Response: ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat does DNA stand for?\\n\\n### Input:\\n\\n\\n### Response:\\nDNA stands for deoxyribonucleic acid.\\n<|endoftext|>'] \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_samples):\n",
    "    print('Ground Truth:',dataset[i]['text'],'\\n')\n",
    "    print('LLaMA2 7B Response:',llama_responses[i],'\\n')\n",
    "    print('Mistral Response:',mistral_responses[i],'\\n')\n",
    "    print('Phi2 Response:',phi_responses[i],'\\n')\n",
    "    print('-'*150)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:23:06.126015500Z",
     "start_time": "2024-03-25T02:23:06.124016400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Human checked responses. LLAMA: 0.9 Mistral: 0.92 Phi2: 0.92"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temperature Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "574a2b831d9d4b25bb89ec29b56c61cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0.001\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.63347      | 0.80563   \n",
      "Mistral    | 0.00000    | 0.63210      | 0.79742   \n",
      "Phi2       | 0.00000    | 0.63802      | 0.80226   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24d2b76c434940e1a9f73cc3579eefd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0.25\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.62554      | 0.79959   \n",
      "Mistral    | 0.00000    | 0.63716      | 0.79905   \n",
      "Phi2       | 0.00000    | 0.64338      | 0.80494   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01d579dadeba40b285eced9eee55e262"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0.5\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.63729      | 0.80476   \n",
      "Mistral    | 0.00000    | 0.61920      | 0.79508   \n",
      "Phi2       | 0.00000    | 0.63706      | 0.80173   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4568c315c074d139af2364954005d8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV: 0.75\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.61024      | 0.78862   \n",
      "Mistral    | 0.00000    | 0.62698      | 0.79942   \n",
      "Phi2       | 0.00000    | 0.62472      | 0.79878   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the parameters to try\n",
    "temperature_values = [0.001, 0.25, 0.5, 0.75]\n",
    "\n",
    "for tv in temperature_values:\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_llama7b_alpaca_ft\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "    list_dataset = make_inst(dataset)\n",
    "    llama_responses = []\n",
    "    num_samples = 20\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, temperature=tv,do_sample=True)\n",
    "        llama_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_mistral\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # dataset = make_inst(dataset)\n",
    "    mistral_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, temperature=tv,do_sample=True)\n",
    "        mistral_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    # Reload model in FP16 and merge it with LoRA weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    #Reload the Base Model and load the QLoRA adapters\n",
    "    model = PeftModel.from_pretrained(model, new_model)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Reload tokenizer to save it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # Run text generation pipeline with our model\n",
    "    #Input Prompt\n",
    "\n",
    "    #prompt = \"What is a large language model?\"\n",
    "    #Wrap the prompt using the right chat template\n",
    "    phi_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, temperature=tv,do_sample=True)\n",
    "        phi_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from rouge import Rouge\n",
    "    from bert_score import score as bert_score\n",
    "    import numpy as np\n",
    "\n",
    "    # llama_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # mistral_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # phi_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    model_scores = {'LLaMA2 7B':{'bleu':[],'rouge':[],'BertF1':[]},'Mistral':{'bleu':[],'rouge':[],'BertF1':[]},'Phi2':{'bleu':[],'rouge':[],'BertF1':[]}}\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # LLaMA2 Evaluation Code\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), llama_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge = Rouge()\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], llama_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], llama_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['LLaMA2 7B']['bleu'].append(bleu_score)\n",
    "        model_scores['LLaMA2 7B']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['LLaMA2 7B']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), mistral_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], mistral_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], mistral_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Mistral']['bleu'].append(bleu_score)\n",
    "        model_scores['Mistral']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Mistral']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), phi_responses[i][0].split(' '))\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], phi_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], phi_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Phi2']['bleu'].append(bleu_score)\n",
    "        model_scores['Phi2']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Phi2']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "    print('TV:',tv)\n",
    "    print(f\"{'Model':<10} | {'Bleu Score':<10} | {'Rouge Score':<12} | {'BertF1':<10}\")\n",
    "    print('-'*45)\n",
    "    for model, scores in model_scores.items():\n",
    "        print(f\"{model:<10} | {np.array(scores['bleu']).mean():<10.5f} | {np.array(scores['rouge']).mean():<12.5f} | {np.array(scores['BertF1']).mean():<10.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:23:30.038425100Z",
     "start_time": "2024-03-25T03:14:35.051697800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e92f21d962834909b76b90f69a587656"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Who is the president of Atlantis? \\nTopic: Politics\\n<|endoftext|>']"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Reload model in FP16 and merge it with LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "#Reload the Base Model and load the QLoRA adapters\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "# Run text generation pipeline with our model\n",
    "#Input Prompt\n",
    "\n",
    "#prompt = \"What is a large language model?\"\n",
    "#Wrap the prompt using the right chat template\n",
    "\n",
    "inputs = tokenizer('Who is the president of Atlantis?', return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, temperature=1,do_sample=True)\n",
    "tokenizer.batch_decode(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:44:48.663489100Z",
     "start_time": "2024-03-25T03:44:43.183429200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beam Size Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93d6a00a3c284166b655cf1cd8b8984d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Size: 1\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.62731      | 0.79732   \n",
      "Mistral    | 0.00000    | 0.60719      | 0.79311   \n",
      "Phi2       | 0.00000    | 0.59868      | 0.77311   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "628dd07b201b4315ac07daf196128412"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Size: 2\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.63415      | 0.79749   \n",
      "Mistral    | 0.00000    | 0.63944      | 0.79521   \n",
      "Phi2       | 0.00000    | 0.64762      | 0.80356   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63622cab7ceb40108934cfec2f403834"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Size: 4\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.63459      | 0.79710   \n",
      "Mistral    | 0.00000    | 0.63602      | 0.79331   \n",
      "Phi2       | 0.00000    | 0.64330      | 0.80417   \n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n",
      "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b56a13c5ab7f46c4898610454fe01196"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Size: 8\n",
      "Model      | Bleu Score | Rouge Score  | BertF1    \n",
      "---------------------------------------------\n",
      "LLaMA2 7B  | 0.00000    | 0.62951      | 0.79618   \n",
      "Mistral    | 0.00000    | 0.62115      | 0.79301   \n",
      "Phi2       | 0.00000    | 0.64089      | 0.80295   \n"
     ]
    }
   ],
   "source": [
    "# Define the parameters to try\n",
    "beam_sizes = [1, 2, 4, 8]\n",
    "\n",
    "for bs in beam_sizes:\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_llama7b_alpaca_ft\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "    list_dataset = make_inst(dataset)\n",
    "    llama_responses = []\n",
    "    num_samples = 20\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=bs,do_sample=True)\n",
    "        llama_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_mistral\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # dataset = make_inst(dataset)\n",
    "    mistral_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=bs,do_sample=True)\n",
    "        mistral_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    # Reload model in FP16 and merge it with LoRA weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    #Reload the Base Model and load the QLoRA adapters\n",
    "    model = PeftModel.from_pretrained(model, new_model)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Reload tokenizer to save it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # Run text generation pipeline with our model\n",
    "    #Input Prompt\n",
    "\n",
    "    #prompt = \"What is a large language model?\"\n",
    "    #Wrap the prompt using the right chat template\n",
    "    phi_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, num_beams=bs, do_sample=True)\n",
    "        phi_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from rouge import Rouge\n",
    "    from bert_score import score as bert_score\n",
    "    import numpy as np\n",
    "\n",
    "    # llama_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # mistral_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # phi_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    model_scores = {'LLaMA2 7B':{'bleu':[],'rouge':[],'BertF1':[]},'Mistral':{'bleu':[],'rouge':[],'BertF1':[]},'Phi2':{'bleu':[],'rouge':[],'BertF1':[]}}\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # LLaMA2 Evaluation Code\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), llama_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge = Rouge()\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], llama_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], llama_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['LLaMA2 7B']['bleu'].append(bleu_score)\n",
    "        model_scores['LLaMA2 7B']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['LLaMA2 7B']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), mistral_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], mistral_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], mistral_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Mistral']['bleu'].append(bleu_score)\n",
    "        model_scores['Mistral']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Mistral']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), phi_responses[i][0].split(' '))\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], phi_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], phi_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Phi2']['bleu'].append(bleu_score)\n",
    "        model_scores['Phi2']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Phi2']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "    print('Beam Size:', bs)\n",
    "    print(f\"{'Model':<10} | {'Bleu Score':<10} | {'Rouge Score':<12} | {'BertF1':<10}\")\n",
    "    print('-'*45)\n",
    "    for model, scores in model_scores.items():\n",
    "        print(f\"{model:<10} | {np.array(scores['bleu']).mean():<10.5f} | {np.array(scores['rouge']).mean():<12.5f} | {np.array(scores['BertF1']).mean():<10.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:34:42.921979200Z",
     "start_time": "2024-03-25T03:23:30.031424800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tok_k Tests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.3\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "51760\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`top_k` has to be a strictly positive integer, but is 0.2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_samples):\n\u001B[1;32m     21\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m tokenizer(list_dataset[i], return_tensors \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     llama_responses\u001B[38;5;241m.\u001B[39mappend(tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(outputs))\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/unsloth/models/llama.py:909\u001B[0m, in \u001B[0;36m_wrap_fast_inference.<locals>._fast_generate\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode\n\u001B[1;32m    907\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fast_generate\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    908\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type \u001B[38;5;241m=\u001B[39m device_type, dtype \u001B[38;5;241m=\u001B[39m dtype):\n\u001B[0;32m--> 909\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/peft/peft_model.py:1190\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.generate\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1188\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1189\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1190\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1192\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/transformers/generation/utils.py:1564\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1545\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_contrastive_search(\n\u001B[1;32m   1546\u001B[0m         input_ids,\n\u001B[1;32m   1547\u001B[0m         top_k\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mtop_k,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1559\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1560\u001B[0m     )\n\u001B[1;32m   1562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mSAMPLE:\n\u001B[1;32m   1563\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m-> 1564\u001B[0m     logits_warper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_logits_warper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1566\u001B[0m     \u001B[38;5;66;03m# 12. expand input_ids with `num_return_sequences` additional sequences per batch\u001B[39;00m\n\u001B[1;32m   1567\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   1568\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1569\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[1;32m   1570\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   1571\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1572\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/transformers/generation/utils.py:731\u001B[0m, in \u001B[0;36mGenerationMixin._get_logits_warper\u001B[0;34m(self, generation_config)\u001B[0m\n\u001B[1;32m    729\u001B[0m     warpers\u001B[38;5;241m.\u001B[39mappend(TemperatureLogitsWarper(generation_config\u001B[38;5;241m.\u001B[39mtemperature))\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mtop_k \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mtop_k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 731\u001B[0m     warpers\u001B[38;5;241m.\u001B[39mappend(\u001B[43mTopKLogitsWarper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_tokens_to_keep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_tokens_to_keep\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mtop_p \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mtop_p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m    733\u001B[0m     warpers\u001B[38;5;241m.\u001B[39mappend(TopPLogitsWarper(top_p\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mtop_p, min_tokens_to_keep\u001B[38;5;241m=\u001B[39mmin_tokens_to_keep))\n",
      "File \u001B[0;32m~/miniconda3/envs/pythonProject2/lib/python3.10/site-packages/transformers/generation/logits_process.py:502\u001B[0m, in \u001B[0;36mTopKLogitsWarper.__init__\u001B[0;34m(self, top_k, filter_value, min_tokens_to_keep)\u001B[0m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, top_k: \u001B[38;5;28mint\u001B[39m, filter_value: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInf\u001B[39m\u001B[38;5;124m\"\u001B[39m), min_tokens_to_keep: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(top_k, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m top_k \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 502\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`top_k` has to be a strictly positive integer, but is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtop_k\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtop_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(top_k, min_tokens_to_keep)\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilter_value \u001B[38;5;241m=\u001B[39m filter_value\n",
      "\u001B[0;31mValueError\u001B[0m: `top_k` has to be a strictly positive integer, but is 0.2"
     ]
    }
   ],
   "source": [
    "# Define the parameters to try\n",
    "tok_ks = [ 2, 4, 8, 16]\n",
    "\n",
    "for tk in tok_ks:\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_llama7b_alpaca_ft\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "    list_dataset = make_inst(dataset)\n",
    "    llama_responses = []\n",
    "    num_samples = 20\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, top_k=tk, do_sample=True)\n",
    "        llama_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    if True:\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"lora_mistral\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = max_seq_length,\n",
    "            dtype = dtype,\n",
    "            load_in_4bit = load_in_4bit,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # dataset = make_inst(dataset)\n",
    "    mistral_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, top_k=tk, do_sample=True)\n",
    "        mistral_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    # Reload model in FP16 and merge it with LoRA weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "    #Reload the Base Model and load the QLoRA adapters\n",
    "    model = PeftModel.from_pretrained(model, new_model)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # Reload tokenizer to save it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    # Run text generation pipeline with our model\n",
    "    #Input Prompt\n",
    "\n",
    "    #prompt = \"What is a large language model?\"\n",
    "    #Wrap the prompt using the right chat template\n",
    "    phi_responses = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        inputs = tokenizer(list_dataset[i], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True, top_k=tk, do_sample=True)\n",
    "        phi_responses.append(tokenizer.batch_decode(outputs))\n",
    "\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from rouge import Rouge\n",
    "    from bert_score import score as bert_score\n",
    "    import numpy as np\n",
    "\n",
    "    # llama_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # mistral_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    # phi_scores = {'bleu':[],'rouge':[],'BertF1':[]}\n",
    "    model_scores = {'LLaMA2 7B':{'bleu':[],'rouge':[],'BertF1':[]},'Mistral':{'bleu':[],'rouge':[],'BertF1':[]},'Phi2':{'bleu':[],'rouge':[],'BertF1':[]}}\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # LLaMA2 Evaluation Code\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), llama_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge = Rouge()\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], llama_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], llama_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['LLaMA2 7B']['bleu'].append(bleu_score)\n",
    "        model_scores['LLaMA2 7B']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['LLaMA2 7B']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), mistral_responses[i][0].split(' '))\n",
    "        # print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], mistral_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], mistral_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Mistral']['bleu'].append(bleu_score)\n",
    "        model_scores['Mistral']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Mistral']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "        bleu_score = sentence_bleu(dataset[i]['text'].split(' '), phi_responses[i][0].split(' '))\n",
    "\n",
    "        rouge_scores = rouge.get_scores(dataset[i]['text'], phi_responses[i][0], avg=True)\n",
    "        # print(f\"Rouge-L Score: {rouge_scores['rouge-l']['f']}\")\n",
    "        P, R, F1 = bert_score([dataset[i]['text']], phi_responses[i], lang=\"en\", verbose=False, model_type='bert-base-uncased')\n",
    "\n",
    "        model_scores['Phi2']['bleu'].append(bleu_score)\n",
    "        model_scores['Phi2']['rouge'].append(rouge_scores['rouge-l']['f'])\n",
    "        model_scores['Phi2']['BertF1'].append(F1.cpu().detach().numpy()[0])\n",
    "\n",
    "    print('Top K:',tk)\n",
    "    print(f\"{'Model':<10} | {'Bleu Score':<10} | {'Rouge Score':<12} | {'BertF1':<10}\")\n",
    "    print('-'*45)\n",
    "    for model, scores in model_scores.items():\n",
    "        print(f\"{model:<10} | {np.array(scores['bleu']).mean():<10.5f} | {np.array(scores['rouge']).mean():<12.5f} | {np.array(scores['BertF1']).mean():<10.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T04:14:17.417160500Z",
     "start_time": "2024-03-25T04:14:09.422397700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonproject2",
   "language": "python",
   "display_name": "pythonProject2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
